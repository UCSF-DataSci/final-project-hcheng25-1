{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6218fe",
   "metadata": {},
   "source": [
    "# Using Prediction Models to Predict Esophageal Squamous Cell Carcinoma ESCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c917b7e",
   "metadata": {},
   "source": [
    "## Background\n",
    "For the past few months, I have been working with case-control data from a study by Mmbaga et al. focusing on a subtype of esophageal cancer called esophageal squamous cell carcinoma (ESCC). This data contains information for ESCC and non-ESCC patients about many potential risk factors, including behavioral, sociodemographic, and medical histories. The secondary study I am contributing to does not seek to train a predictive model and is instead focused on studying possible causal factors, but I was curious if the data could be used to train a predictive model to identify individuals who may be at most risk of developing ESCC. The secondary study has fit a multivariate logistic regression model on the data to study the associations of risk factors with ESCC, but I want to determine how well a logistic regression model can predict outcome using a train/test split. In addition, I would like to see how the other supervised models (random forest, XGBoost, deep learning) can pexgborm after tuning for hyperparameters. The main distinction I am hoping to draw between my work on the secondary study and this project is that for this project, I am less concerned with the interpretation of how the risk factors are associated with ESCC outcome, and instead focused on the potential for a prediction model to identify people who are most at risk of developing ESCC based on the data that has been gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d01a2",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Handling missing data\n",
    "I had previously already cleaned the data using R and outputted it into a `.csv` file for use in analysis in R. There is some data for some variables missing, and in most variables, the missing data does not comprise much of the study population, but for HIV status, roughly 39% of subjects were missing data. Because the training process will require no missing data in the features, I will drop the HIV variable altogether for this particular project and will drop the subjects that are missing any data for any of the other variables, which resulted in the number of subjects being reduced from 932 to 861."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcca07ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject count: 932\n",
      "zone                0.000000\n",
      "casecon             0.000000\n",
      "age                 0.000000\n",
      "gender              0.000000\n",
      "edu                 0.151288\n",
      "occupation          0.000000\n",
      "iwi                 0.023605\n",
      "famhis              0.017167\n",
      "hiv                 0.394850\n",
      "smoking             0.001073\n",
      "tobacco             0.001073\n",
      "secondhandsmoke     0.011803\n",
      "alcohol             0.000000\n",
      "localbrew           0.023605\n",
      "farmwork            0.000000\n",
      "pesticide           0.000000\n",
      "preservation        0.000000\n",
      "treatgrainnuts      0.002146\n",
      "cookingsite         0.003219\n",
      "wellwater           0.000000\n",
      "currentfirewood     0.000000\n",
      "childfiresleep      0.001073\n",
      "burnedtongue        0.001073\n",
      "atesoilclay         0.000000\n",
      "dailyteeth          0.005365\n",
      "beansmagadi         0.001073\n",
      "rawgreensdaily      0.002146\n",
      "fruitsdaily         0.004292\n",
      "smokedfishdaily     0.002146\n",
      "spicychilisdaily    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/cleaned_data.csv')\n",
    "df = df.drop(columns='Unnamed: 0') # drop first column that contains no data\n",
    "print(f'Subject count: {len(df)}')\n",
    "print(df.isna().sum()/len(df)) # examine missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66123385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final subject count: 718\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns='hiv') # drop hiv due to high missingness\n",
    "df = df.dropna() # drop rows that contain missing data\n",
    "print(f'Final subject count: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe746b9",
   "metadata": {},
   "source": [
    "### Preparation of categorical variables\n",
    "In this dataset, all variables except age are categorical, so I will use OneHotEncoder to process them first.\n",
    "\n",
    "I will split the data into 70/30 for training and testing, and I want to use the same split for training all models I fit so that they can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ddf31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casecon</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Control</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Control</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Case</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Control</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>Case</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Control</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Case</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>Control</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>Control</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     casecon  age\n",
       "1    Control   58\n",
       "4       Case   77\n",
       "5    Control   77\n",
       "6       Case   32\n",
       "7    Control   40\n",
       "..       ...  ...\n",
       "926     Case   56\n",
       "927  Control   57\n",
       "928     Case   38\n",
       "929  Control   38\n",
       "931  Control   66\n",
       "\n",
       "[718 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encode = pd.DataFrame(df[['casecon','age']])\n",
    "df_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdffe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_var = df.drop(columns=['casecon', 'age']).columns.tolist() # all variables except casecon (outcome) and age (continuous)\n",
    "\n",
    "df_encode = pd.DataFrame(df[['casecon','age']]) # casecon and age untouched, initialize new data frame\n",
    "\n",
    "for var in cat_var: # for each cat variable\n",
    "    cat_col = pd.DataFrame(df[var]) # extract variable from df\n",
    "    # apply onehotencoder\n",
    "    transform = OneHotEncoder()\n",
    "    encoded = transform.fit_transform(cat_col)\n",
    "    encoded = pd.DataFrame(encoded.toarray())\n",
    "    # new column names\n",
    "    encoded.columns = transform.get_feature_names_out()\n",
    "    # concatenate new columns to encoded df\n",
    "    df_encode = pd.concat([df_encode, encoded], axis=1)\n",
    "\n",
    "# NaNs were generated in casecon and age columns due to encoding transformation\n",
    "# dropping NaN rows brings back the original number of subjects after dropping missing data\n",
    "df_encode = df_encode.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casecon is outcome column\n",
    "x = df_encode.drop(columns='casecon')\n",
    "y = df_encode['casecon']\n",
    "\n",
    "# split 70/30 train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=72, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c9f83",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2834da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324e9df",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cd5d8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.499\n",
      "Accuracy: 0.497\n",
      "Confusion matrix:\n",
      "[[33 54]\n",
      " [29 49]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Case       0.53      0.38      0.44        87\n",
      "     Control       0.48      0.63      0.54        78\n",
      "\n",
      "    accuracy                           0.50       165\n",
      "   macro avg       0.50      0.50      0.49       165\n",
      "weighted avg       0.51      0.50      0.49       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "logreg_pred = logreg_model.predict(x_test)\n",
    "logreg_prob = logreg_model.predict_proba(x_test)\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_test, logreg_prob[:, 1]):.3f}') # the second column of logreg_prob gives the probability of being class 1 aka an ESCC case\n",
    "print(f'Accuracy: {accuracy_score(y_test, logreg_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test, logreg_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test, logreg_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0513c3",
   "metadata": {},
   "source": [
    "## 2. Random Forest\n",
    "For my random forest model, I decided to tune using a grid search method with 5-fold cross-validation. I decided to try multiples of 10 trees in the model, from 10 to 100. I also decided to try setting a maximum depth for those trees of 5, 10, 15, or 20, as well as no maximum depth at all.\n",
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_tuning = RandomForestClassifier(random_state=72)\n",
    "\n",
    "# defining a grid search for parameters\n",
    "parameters = {\n",
    "    'n_estimators': (np.array(range(10))+1)*10, # test number of trees in multiples of 10 from 10 to 100\n",
    "    'max_depth': [None, 5, 10, 15, 20] # test maximum depths as well as no maximum depth\n",
    "}\n",
    "\n",
    "# grid search using 5-fold cross validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_tuning,\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc', # parameters evaluated based on AUC\n",
    "    cv=5 # 5-fold\n",
    ")\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_params = grid_search.best_params_ # save best parameters to a dictionary for use in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6c58f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ced5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using the best parameters obtained\n",
    "rf = RandomForestClassifier(random_state=72, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'])\n",
    "rf_model = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d112f3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ebafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.534\n",
      "Accuracy: 0.539\n",
      "Confusion matrix:\n",
      "[[40 47]\n",
      " [29 49]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Case       0.58      0.46      0.51        87\n",
      "     Control       0.51      0.63      0.56        78\n",
      "\n",
      "    accuracy                           0.54       165\n",
      "   macro avg       0.55      0.54      0.54       165\n",
      "weighted avg       0.55      0.54      0.54       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf_model.predict(x_test)\n",
    "rf_prob = rf_model.predict_proba(x_test)\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_test, rf_prob[:, 1]):.3f}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, rf_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test, rf_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test, rf_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f5369",
   "metadata": {},
   "source": [
    "## 3. XGBoost\n",
    "For XGBoost, I decided to tune the same parameters as in the Random Forest with the additional parameter of learning rate. Because XGBoost requires the outcome to be coded as 0/1 instead of using the strings as categories, I made a new variable converting `y_train` and `y_test` to a numeric form for use in XGBoost.\n",
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5333c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train and y_test for use in XGBoost\n",
    "y_train_bin = y_train.map({'Case':1, 'Control':0})\n",
    "y_test_bin = y_test.map({'Case':1, 'Control':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_tuning = XGBClassifier(random_state=72)\n",
    "\n",
    "# defining a grid search for parameters\n",
    "parameters = {\n",
    "    'n_estimators': (np.array(range(10))+1)*10, # test number of trees in multiples of 10 from 10 to 100\n",
    "    'max_depth': [None, 5, 10, 15, 20], # test maximum depths as well as no maximum depth\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# grid search using 5-fold cross validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_tuning,\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc', # parameters evaluated based on AUC\n",
    "    cv=5 # 5-fold\n",
    ")\n",
    "grid_search.fit(x_train, y_train_bin)\n",
    "best_params = grid_search.best_params_ # save best parameters to a dictionary for use in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972d285",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8809fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using the best parameters obtained\n",
    "xgb = XGBClassifier(random_state=72, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], learning_rate=best_params['learning_rate'])\n",
    "xgb_model = xgb.fit(x_train, y_train_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a5df0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.487\n",
      "Accuracy: 0.497\n",
      "Confusion matrix:\n",
      "[[58 20]\n",
      " [63 24]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.74      0.58        78\n",
      "           1       0.55      0.28      0.37        87\n",
      "\n",
      "    accuracy                           0.50       165\n",
      "   macro avg       0.51      0.51      0.47       165\n",
      "weighted avg       0.51      0.50      0.47       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = xgb_model.predict(x_test)\n",
    "xgb_prob = xgb_model.predict_proba(x_test)\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_test_bin, xgb_prob[:, 1]):.3f}')\n",
    "print(f'Accuracy: {accuracy_score(y_test_bin, xgb_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test_bin, xgb_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test_bin, xgb_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801d012",
   "metadata": {},
   "source": [
    "## 4. Deep Learning (Neural Networks)\n",
    "While my understanding of neural networks indicates to me that it would not make for the best predictive model choice in my application, I wanted to try building a model anyway just to apply what we learned in the course and see how it performs. My data is not particularly large relative to the data that neural networks are typically used for, so I will be using two layers with lower numbers of neurons (16 and 8). I'll be applying a dropout layer between the layers to help prevent overfitting to the training data. I will then evaluate using the test dataset as I did with previous models.\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f25d98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Build the model\n",
    "nn_model = Sequential([\n",
    "    Dense(16, activation='relu', input_dim=x_train.shape[1]),  # .shape[1] gives the number of encoded variables in x_train\n",
    "    Dropout(0.15),  # dropout to prevent overfitting\n",
    "    Dense(8, activation='relu'), # second layer\n",
    "    Dense(1, activation='sigmoid')  # output for binary outcome\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692d744",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4909248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "AUC: 0.595\n",
      "Accuracy: 0.473\n",
      "Confusion matrix:\n",
      "[[78  0]\n",
      " [87  0]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64        78\n",
      "           1       0.00      0.00      0.00        87\n",
      "\n",
      "    accuracy                           0.47       165\n",
      "   macro avg       0.24      0.50      0.32       165\n",
      "weighted avg       0.22      0.47      0.30       165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hcheng25/final-project-hcheng25-1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hcheng25/final-project-hcheng25-1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hcheng25/final-project-hcheng25-1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "nn_prob = nn_model.predict(x_test)\n",
    "nn_pred = (nn_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_test, nn_prob):.3f}')\n",
    "print(f'Accuracy: {accuracy_score(y_test_bin, nn_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test_bin, nn_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test_bin, nn_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0887dea",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9e34d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Logistic Regression-----\n",
      "AUC: 0.499\n",
      "Accuracy: 0.497\n",
      "Confusion matrix:\n",
      "[[33 54]\n",
      " [29 49]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Case       0.53      0.38      0.44        87\n",
      "     Control       0.48      0.63      0.54        78\n",
      "\n",
      "    accuracy                           0.50       165\n",
      "   macro avg       0.50      0.50      0.49       165\n",
      "weighted avg       0.51      0.50      0.49       165\n",
      "\n",
      "-----Random Forest-----\n",
      "AUC: 0.534\n",
      "Accuracy: 0.539\n",
      "Confusion matrix:\n",
      "[[40 47]\n",
      " [29 49]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Case       0.58      0.46      0.51        87\n",
      "     Control       0.51      0.63      0.56        78\n",
      "\n",
      "    accuracy                           0.54       165\n",
      "   macro avg       0.55      0.54      0.54       165\n",
      "weighted avg       0.55      0.54      0.54       165\n",
      "\n",
      "-----XGBoost-----\n",
      "AUC: 0.487\n",
      "Accuracy: 0.497\n",
      "Confusion matrix:\n",
      "[[58 20]\n",
      " [63 24]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.74      0.58        78\n",
      "           1       0.55      0.28      0.37        87\n",
      "\n",
      "    accuracy                           0.50       165\n",
      "   macro avg       0.51      0.51      0.47       165\n",
      "weighted avg       0.51      0.50      0.47       165\n",
      "\n",
      "-----Neural Network-----\n",
      "AUC: 0.595\n",
      "Accuracy: 0.473\n",
      "Confusion matrix:\n",
      "[[78  0]\n",
      " [87  0]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64        78\n",
      "           1       0.00      0.00      0.00        87\n",
      "\n",
      "    accuracy                           0.47       165\n",
      "   macro avg       0.24      0.50      0.32       165\n",
      "weighted avg       0.22      0.47      0.30       165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hcheng25/final-project-hcheng25-1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hcheng25/final-project-hcheng25-1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hcheng25/final-project-hcheng25-1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "print('-----Logistic Regression-----')\n",
    "print(f'AUC: {roc_auc_score(y_test, logreg_prob[:, 1]):.3f}') # the second column of logreg_prob gives the probability of being class 1 aka an ESCC case\n",
    "print(f'Accuracy: {accuracy_score(y_test, logreg_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test, logreg_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test, logreg_pred)}')\n",
    "\n",
    "print('-----Random Forest-----')\n",
    "print(f'AUC: {roc_auc_score(y_test, rf_prob[:, 1]):.3f}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, rf_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test, rf_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test, rf_pred)}')\n",
    "\n",
    "print('-----XGBoost-----')\n",
    "print(f'AUC: {roc_auc_score(y_test_bin, xgb_prob[:, 1]):.3f}')\n",
    "print(f'Accuracy: {accuracy_score(y_test_bin, xgb_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test_bin, xgb_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test_bin, xgb_pred)}')\n",
    "\n",
    "print('-----Neural Network-----')\n",
    "print(f'AUC: {roc_auc_score(y_test, nn_prob):.3f}')\n",
    "print(f'Accuracy: {accuracy_score(y_test_bin, nn_pred):.3f}')\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(y_test_bin, nn_pred)}')\n",
    "print(f'Classification report:\\n{classification_report(y_test_bin, nn_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273337c",
   "metadata": {},
   "source": [
    "In order of AUC, the neural network at first glance appears to perform best, followed by the Random Forest model, then logistic regression, and finally XGBoost. However, even before evaluating other metrics, I noticed that the neural network did not predict any cases in my test data; it only predicted controls, and happened to get it right 47% of the time because 47% of the test data were controls. I expected neural networks to perform quite poorly as a predictor if only because my data was small; overfitting was my primary concern through the process of coding and building the model. I feel that the prediction of only controls across the entire dataset confirms my concerns that there probably isn't enough data for the neural network to learn and predict ESCC outcomes properly\n",
    "\n",
    "While evaluating which model I would choose to use as a predictor, I compared XGBoost and Random Forest relative to the logistic regression model because it is one of the simpler models that can be fit to this data and because I had already been working with a logistic regression model with this data, albeit not in a train/test split scenario. XGBoost and the logistic regression prediction models had very similar AUC and accuracy metrics, while the Random Forest model had higher AUC and accuracy than both.\n",
    "\n",
    "In the context of predicting which subjects are most at risk of ESCC based on the features input into the prediction model, I feel that higher recall is more important than higher precision. Higher recall means that more high-risk individuals are being identified by the model whereas higher precision means that more of the flagged individuals are truly high risk. Because it would be more important to get high risk individuals to be evaluated by a healthcare professional even if some individuals who are not high risk are also recommended to see a doctor, I would weigh the recall metric more than the precision metric. For this reason, I feel the Random Forest would be the best of these models for this application because it has a higher recall score of 0.46 for cases compared to logistic regression (0.38) and XGBoost (0.28). Even then, I would still not feel comfortable implementing this model; a recall score of 0.46 means that only 46% of high-risk individuals are being identified for referral to a doctor, which, while better than none of them seeing a doctor, is still quite poor in performance in a clinical context.\n",
    "\n",
    "There are also sources of bias in this data that would not make the model ideal for use in the real world. The data was gathered from a case-control study that selected based on outcome and was matched based on age and gender, so there are already several likely sources of bias in the training of the models. Because ESCC in Tanzania is still relatively unstudied, it is much too early to build a predictive model of this nature to aid in public health interventions. However, I am hopeful that as more studies are performed, this kind of predictive modeling may play a useful role in lessening the burden of ESCC in the population Tanzania.\n",
    "\n",
    "## Refences\n",
    "Mmbaga EJ, Deardorff K V, Mushi B, Mgisha W, Merritt M, Hiatt RA, et al. Characteristics of esophageal cancer cases in Tanzania. J Glob Oncol. 2017;4:1–10.\n",
    "\n",
    "Gabel J V, Chamberlain RM, Ngoma T, Mwaiselage J, Schmid KK, Kahesa C, et al. Clinical and epidemiologic variations of esophageal cancer in Tanzania. World J Gastrointest Oncol. 2016;8(3):314.\n",
    "\n",
    "Globocan data [Internet]. Available from: https://gco.iarc.fr/today/data/factsheets/populations/834-tanzania-united-republic-of-fact-sheets.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
